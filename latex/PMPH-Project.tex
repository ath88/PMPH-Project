\documentclass[11pt]{article}

%---- defitions ----
\def\Title{Programming Massively Parallel Hardware\\
\vspace{1.5cm}
\textbf{Group Project}}
\def\Author{Esben Skaarup, Asbj\o rn Thegler \& \'{A}sbj\o rn Vider\o \ J\o kladal}

%---- packages ----
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{courier}
% \usepackage{listings}
\usepackage[pdftex,colorlinks=true]{hyperref}
\usepackage{graphics}
\usepackage[disable]{todonotes}

\usepackage{color}
\definecolor{ublue}{rgb}{0,0,0.5}
\definecolor{ugreen}{rgb}{0,0.5,0}
\definecolor{ured}{rgb}{0.5,0,0}
\definecolor{ugrey}{rgb}{0.5,0.5,0.5}

\usepackage{listings}
\lstset{
	language=C,			% choose the language of the code
	% numbers=left,				% where to put the line-numbers
	numberstyle=\tiny,			% line-numbers font size
	stepnumber=1,				% the step between two line-numbers
%	numbersep=10pt,				% how far the line-numbers are from the code
	basicstyle= \ttfamily \footnotesize, %\small %\scriptsize, % the size of the fonts for the code
%	breaklines=true,
	tabsize=4,
	identifierstyle=\color{black},
	keywordstyle=\color{ublue}\bf,
	stringstyle=\color{ured},
	commentstyle=\color{ugreen},
	showstringspaces=false,		% underline spaces within strings
	xleftmargin=.1\textwidth, % left margin
	xrightmargin=.1\textwidth, % right margin
%	inputencoding=utf8x,
%	extendedchars= true
%	frame=single,
}

\begin{document}
\title{\Title}
\author{\Author}
\date{\today}
\maketitle

\section{Introduction}
\todo{V}
In this assignment, we have parallelized an implementation of volatility calibration using the Crank-Nicolson finite difference method.

We have chosen to first reason about the original sequential implementation. 
Next, we reason about and explain how we created our OpenMP 
implementation. Finally, we reason about our CUDA implementation, and compare
it to the original implementation and the OpenMP implementation.

\section{Sequential Implementation}
\todo{V}
First we will profile the original implementation, and showcase our findings.
This will help us determine what we definitely should parallelize, and what is
less relevant.

\subsection{Timing}
\todo{V}
We have made approximate timing of specific parts of the original 
implementation, to get an overview 
of where we can achieve relevant speedups according to Amdahl's Law. The results
can be seen in \autoref{table:origtime}. In the table, the indentations 
in the first column indicate what parts are inside other parts. For example,
the running time of tridag\_0 is part of the running time of rollback\_2, which
in turn is part of rollback, and so forth. The time of the outer loop is the 
total running time of the entire calculation. 

\autoref{table:origtime} is based on a single execution of the implementation, 
but we have visually verified that it is an average example. 
The table clearly shows what parts of the implementation we should parallelize.
We see that, combined, the \texttt{updateParams} function and the \texttt{rollback} function take approximately $56.83\%+42.83\%=99.66\%$ of the execution time.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
Name \textbackslash\ Dataset & Small      & Small \% & Medium     & Medium \% & Large        & Large \% \\ \hline
outer                        & 2050766 $\mu s$ & 100.00\% & 4240619 $\mu s$ & 100.00\%  & 187729378 $\mu s$ & 100.00\% \\ \hline
\ \ init                     & 171 $\mu s$     & 0.00\%   & 238 $\mu s$     & 0.00\%    & 2068 $\mu s$      & 0.00\%   \\ \hline
\ \ setPayoff                & 174 $\mu s$     & 0.00\%   & 363 $\mu s$     & 0.00\%    & 10651 $\mu s$     & 0.01\%   \\ \hline
\ \ updateParams             & 1199969 $\mu s$ & 58.15\%  & 2492123 $\mu s$ & 58.77\%   & 106691216 $\mu s$ & 56.83\%  \\ \hline
\ \ rollback                 & 839027 $\mu s$  & 40.91\%  & 1729438 $\mu s$ & 40.78\%   & 80412624 $\mu s$  & 42.83\%  \\ \hline
\ \ \ \ rollback\_0          & 117198 $\mu s$  & 5.71\%   & 246938 $\mu s$  & 5.82\%    & 11824914 $\mu s$  & 6.30\%   \\ \hline
\ \ \ \ rollback\_1          & 111773 $\mu s$  & 5.45\%   & 236186 $\mu s$  & 5.56\%    & 12284147 $\mu s$  & 6.54\%   \\ \hline
\ \ \ \ rollback\_2          & 286646 $\mu s$  & 13.98\%  & 585957 $\mu s$  & 13.82\%   & 25915774 $\mu s$  & 13.80\%  \\ \hline
\ \ \ \ \ \ tridag\_0        & 203364 $\mu s$  & 9.92\%   & 421277 $\mu s$  & 9.93\%    & 18500994 $\mu s$  & 9.86\%   \\ \hline
\ \ \ \ rollback\_3          & 304214 $\mu s$  & 14.83\%  & 627087 $\mu s$  & 14.79\%   & 28554403 $\mu s$  & 15.21\%  \\ \hline
\ \ \ \ \ \ tridag\_1        & 200502 $\mu s$  & 9.78\%   & 416905 $\mu s$  & 9.83\%    & 18612169 $\mu s$  & 9.91\%   \\ \hline
\end{tabular}
}
\caption{Approximate timings of the original implementation}
\label{table:origtime}
\end{table}

A bit of terminology: We refer to the following loop
\begin{lstlisting}[language=C]
  for(unsigned i = 0; i < outer; ++ i) { ... }
\end{lstlisting}
as just the \emph{outer}-loop. This loop
\begin{lstlisting}[language=C]
  for(int i = numT-2;i>=0;--i) { ... }
\end{lstlisting}
we call the \emph{timeline}-loop, where we have defined
\begin{lstlisting}[language=C]
  numT = globs.myTimeline.size().
\end{lstlisting}
And finally, these two loops
\begin{lstlisting}[language=C]
  for(i=0;i<numX;i++) { ... }
  for(j=0;j<numY;j++) { ... }
\end{lstlisting}
are called the \emph{x}-loop and the \emph{y}-loop, respectively.

Basically all of the program runs inside the outer-loop.
In this loop, first there is some initialization, and then we enter the timeline-loop in which the \texttt{updateParams} function and the \texttt{rollback} function are called.
Here, the \texttt{updateParams} function consists of a nested for-loop (an y-loop inside an x-loop), and the \texttt{rollback} function runs several combinations of the x-loop and the y-loop, some of which call the \texttt{tridag} function.

So our efforts have been put into finding out which of these loops are (or can be made) parallel, and, in the case of the CUDA implementation, distributing and interchanging the parallel loops in order to create perfect loop-nests, which then correspond to kernels.


\subsection{Validation}
\todo{V}
The original implementation validates against all 3 datasets. This is expected,
since we did not change the original implementation. It is assumed that the 
original implementation is therefore correct.


\section{OpenMP Implementation}
\todo{V T}
In this section we will first reason about how we transformed the sequential
implementation to run on multiple cores. Then we will reason about the validity 
of our implementation, and finally we will compare it to the original 
implementation.

\subsection{Privatization}
\todo{V}
The outer-loop in the original implementation uses the same C 
struct for each
iteration of the loop. This, in turn, makes the loop inherently non-parallel, since,
if executed in parallel, all iterations would be writing to the same memory location. 
We can, however, give each iteration its own copy of the
struct, so that the iterations do not read and write to the same memory location. This is 
known as privatization, and allows us to parallelize the entire loop
with an OpenMP pragma directive. 

While privatization does work in this particular case, it is not immediately clear why it does. It is
important to note that it would not have worked if there had been a dependency between the iterations,
e.g., an accumulator that was used by the next iteration. The only way to ensure
that this is not the case is to look through the code and take note of which 
variables are read, and verify that they are all previously written in the 
same iteration, such that nothing is carried from each loop iteration.

When telling OpenMP how to parallelize, we decided to go with static scheduling.
This means that all iterations are distributed before executing the loop. This gives
less overhead, compared to dynamic scheduling, which adjusts to the workload at runtime. We do
this because we know that each iteration of the outer loop yields the same
amount of work, such that no iteration takes much longer time than the others.
Had they been of different size, then static scheduling would lead to load 
imbalance, and incur extra overhead due to significant idling on some cores.


\subsection{Validation}
\todo{V}
The implementation validates against all 3 datasets. This shows that our
implementation is not catastrophically wrong, but does not \emph{prove} that it is
correct. However, we make the same assumption about correctness as with the 
original implementation: The fact that it validates against all 3 datasets is sufficient evidence of correctness.

We have briefly looked at how the algorithm works and what it does. We
realize that if we have a wrong implementation, then the results should differ quite much from the correct results, provided we have a relatively high amount of timesteps. We created our own ``Tiny'' dataset and ran it on the original implementation to produce ``correct'' output data for it. It turned out that some implementations validated on this dataset, even though they actually failed on the other datasets.

Another curiosity is that, if we skip writing to the result array, then there
is a good chance that we end up allocating the exact same block of memory as the 
previous execution, and that the results are already there. This happened to us
at least once, and it can be fixed by 0-initializing the result array prior to
the calculation.


\subsection{Speedup}
\todo{V}
While this implementation is parallel across multiple processors, measuring the
time taking of specific parts of the implementation is harder. We can, however, 
compare with the total running time, and the results can be seen in 
\autoref{table:omptime}.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|}
\hline
Speedup \textbackslash\ Dataset & Small      & Medium     & Large        \\ \hline
Original                        & 2050766 $\mu s$ & 4240619 $\mu s$ & 187729378 $\mu s$ \\ \hline
OpenMP                          & 184573 $\mu s$  & 235295 $\mu s$  & 9150758 $\mu s$   \\ \hline
Speedup                         & 11.11      & 18.02      & 20.52        \\ \hline
\end{tabular}
}
\caption{Speedup comparison of the original implementation and the OpenMP implementation}
\label{table:omptime}
\end{table}

\subsection{Conclusion}
\todo{V}
When using OpenMP, we utilize the machine's 16 cores with hyperthreading. 
This will give us a maximum realistic speedup of factor 32.
However, we did expect the actual speedup to be closer to 16 than 32, since hyperthreading does not change the fact that we have 16 hardware threads, it only helps hide latencies a bit.

When running the small 
dataset, the OpenMP pragma we used will only create 16 processes on 16 cores
corresponding to the number of iterations on the outer loop. This means that 
we will only get a max speedup of factor 16 on the small dataset. 

When using 16 cores, we expect a speedup of slightly less than 16. Then we 
gain a little extra from hyperthreading, if we have more than 16 processes. 
Finally we loose a little as overhead from creating and sharing result memory.
This makes the results very much in line with our expectations.


\section{CUDA Implementation}
\todo{V}
In this section we will first reason about how we transformed the sequential
implementation to run on the GPU. Then we will reason about the validity of 
our implementation, and finally we will compare it to the original 
implementation and the OpenMP implementation.

\subsection{Transformations}
\todo{V}
We did various transformations on the original code. Here we explain what we 
did in approximately chronological order.

\subsubsection{Array Expansion}
\todo{V}
The outer loop is not parallel as it is. Previously we remedied this by privatization,
but this approach will not work here, since we aim to use the GPU where
it is preferable to do all memory allocation before launching the CUDA-kernels.
In addition, we need to keep variables available across multiple kernels. Therefore we
expand most of the arrays in the struct with $O$ copies, where $O$ corresponds to
the number of iterations in the outer loop. The arrays we have expanded are:
\texttt{myResult}, \texttt{myVarX}, \texttt{myVarY}, \texttt{u}, \texttt{v}, \texttt{a}, \texttt{b}, \texttt{c}, \texttt{y} and \texttt{yy}. Those were chosen because 
they are all being written to inside the loop. The remaining arrays are only 
read from, and will thus not result in a conflict. 

After the array expansion,
it was necessary to ensure that each iteration used its own part of the newly
expanded array. This is done by indexing into the expanded arrays, based on the
iteration counter, for each iteration. When the indexing into the expanded
array is correct, then we have successfully made the outer-loop parallel.

\subsubsection{Loop Distribution}
\todo{V}
When we have established that a loop is parallel, we can make use of the theorem which says that a parallel loop can be distributed across its statements.
So if we have a parallel loop containing statements A, B and C, we can transform this into three separate loops ---
one that loops over A, a second one that loops over B, and a third one that loops over C.
The only condition is that we must remember to array-expand all private variables that span two or more of the newly created loops (otherwise, each iteration of one loop would just overwrite the variable such that only the value from the last thread to finish would be available in the next loop, which is obviously not what we want).

At this point, we have a parallel outer-loop that has some initialization statements (\texttt{initGrid} and \texttt{initOperator}), and then a sequential loop (the timeline-loop).
So we distribute the outer-loop, giving us one outer-loop containing the initialization statements, and another outer-loop containing just the timeline-loop.
After distributing, we can continue the transformations on both new copies of the outer-loop.


\subsubsection{Loop Interchange}
\todo{V}
At this point in our sequence of transformations, in addition to an initialization loop, the program consists of the parallel outer-loop with the sequential timeline-loop inside of it.
Now we can make use of the fact that a parallel loop can be interchanged inwards.
So we do a loop interchange, such that the sequential timeline-loop now becomes the outermost loop.
Now we have the parallel outer-loop on the inside of the sequential loop (note that we still call it the ``outer''-loop, because it iterates over the ``outer'' parameter), and then we can continue applying loop distribution and loop interchange until our program has the desired structure.
As previously mentioned, often we want to end up with perfect loop-nests that correspond to CUDA-kernels.

\subsubsection{Approach to Parallelization}
\label{sec:approach}
In this case, the goal isn't necessarily to end up with loop-nests \emph{per se}. So to put it more precise, we want parallel loops with enough iterations (i.e., threads) to fully utilize the GPU.
So single loops are also perfectly fine, as long as their sizes are large enough.
In our datasets, however, no single dimension is large enough to fully occupy the GPU, so we do need to create loop-nests in order to obtain sufficiently large loops.

Currently, we have only optimized our program towards the large dataset.
After performing the transformations mentioned above, in much of the code we end up with triple-nested loops: An outer-loop, an x-loop and a y-loop. Since each of these loops are parallel, such nests might be interchanged in any way we want. For the large dataset, it is sufficient to parallelize along two of the three dimensions.
For the large dataset, the two loops make up $\mathtt{outer} \cdot \mathtt{num\_y} = 128 \cdot 256 = 32,768$ threads.
The card can occupy $20 \cdot 1024 = 20480$ threads.
Thus, the parallelization of the two loops is enough to utilize the GPU, and further parallelization would not increase the amount of work done in parallel.
For the medium dataset, it might have been interesting to try parallelizing a third dimension, but we managed reasonable speedups without it.
For the small dataset we would need a very different approach to obtain good speedups.
This is something we have only reasoned about with pen and paper calculations --- we didn't do any experiment parallelizing the third dimension to see what the actual impact was.

As a consequence, we have only made two-dimensional kernels at this point. We have chosen to parallelize along the outer and y dimensions where possible. Some places in the code, the transformations only gave doubly-nested loops, and if those consisted of an outer-loop and an x-loop, then naturally we parallelize along the outer and x dimensions instead.

\subsubsection{Coalesced Data Access}
After having parallelized the execution, the data accesses were a major bottleneck because the they were not coalesced.
The problems were mostly $x$ by $y$ matrices that were traversed in the opposite order on the GPU because we have parallelized the $y$ loop.
When traversing a matrix the access pattern should look like this (where \texttt{i} traverses the $x$-axis in the loop and \texttt{j} traverses the $y$-axis in parallel):
\begin{lstlisting}[language=C]
for(int i = 0; i < numX; i++) {
	data[i*numY + j];
}
\end{lstlisting}

Since most of the matrices are always traversed in the same direction, we could simply swap the indexing at every access to the matrix, resulting in major speedups without adding any code.

Two matrices, namely \texttt{myResult} and \texttt{y} were, however, traversed in different directions at different places in the loop.
For these, we inserted kernels to transpose them between kernel calls, so that each kernel can traverse the data in an optimal manner and still get the desired results.
We used the tiled transpose kernel from assignment 3 for this (modified to take the outer loop into account), and the time spent transposing is much less than the time saved using coalesced access.
Only on the small dataset is the time spent transposing a dominating contribution to the total time.

\todo{V}

\subsubsection{Nudging around}

After applying the main transformations, the main speedups were already obtained, and the performance was acceptable.
However, as the most significant bottlenecks were removed, a handful of minor optimizations would suddenly lead to relevant speedups.
In the following, we will describe the various optimizations we performed to tweak the implementation.

\todo{V T}

\paragraph{Duplicate Data Access\\}
At several places in the code, some data was accessed several times close to each other, but not close enough to be optimized by the compiler (if it attempts it at all).
We obtained significant speedups by caching this data in local variables.

Similarly, there were places where distinct data elements was located closely in global memory was read at different times in a kernel.
Moving these reads to occur at the same place and saving them in local variables for later use, we obtained better use of the memory cache, resulting in noticeable speedups.

\paragraph{Shared Memory\\}
Most of the matrices are too big to fit in the shared memory, and most of the reads differ from thread to thread, making shared memory hard to utilize in this problem.

However, a few places we read \texttt{myDxx} and \texttt{myDyy} into shared memory, reducing the amount of global memory transactions.
This was possible because of the problem sizes and the number of threads per block, and is thus not a general solution.
However, even the big dataset was compatible to this solution.
For a general solution, it would be easy to automatically disable this optimization when it is inappropriate.

Another use of shared memory is the transpose kernel, which yields a massive speedup from this.
Since this is described in detail in assignment 3, we will not discuss it further here.

\paragraph{Block Dimensions\\}

The block dimensions play a large role in the overall performance, as they determine the occupancy of the multiprocessors.
Since we have mainly been struggling with memory issues during this project, we would expect that the maximum number of threads was optimal, since it gives each multiprocessor the best opportunity for hiding latencies.
Through experimentation, we determined that the \texttt{rollback()} functions ran most effectively with 24 by 24 blocks, that is 576 threads per multiprocessor.
The \texttt{tridag()} functions are optimal with the maximum 32 by 32 configurations, as is transposition functions (as we already knew from assignment 3).

\paragraph{Constant Memory\\}

The constant memory can be used to optimize memory reads because it is faster than global memory.
However, it is very small, and thus hard to utilize for this project.
We did not have the time to experiment with this, but we expect that only minor optimizations can be done, as most of the memory read time is spent on the largest matrices, which can not fit in constant memory.

\paragraph{Array Indexing\\}

Calculation of indices for matrix lookups can be optimized by maintaining a pointer offset for a loop instead of calculating the address based on the loop counter.
This can save a small amount of time, and is mostly relevant for loops with little works but many iterations.
In our case, we were able to optimize some of the loops in this fashion, but most of them showed no measurable difference.
A few even became slower, which is why some of the loop are left without this optimization.
We suspect that this is because the compiler can sometimes optimize the loop, and by manually optimizing, one can sometimes make it harder for the compiler to apply its own optimizations.

\subsection{Validation}
\todo{V}
This implementation validates against all 3 datasets. As mentioned earlier, this does not strictly prove anything. However, we make the same assumption about correctness as with the 
original implementation and the OpenMP implementation: The fact that it 
validates is sufficient evidence of correctness.

\subsection{Speedup}
\todo{V}
The speedup and relative factors can be seen in \autoref{table:cudatime}. 
We have compared the execution time of the CUDA implementation with the 
original implementation, and with the OpenMP implementation. 

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|}
\hline
Speedup \textbackslash\ Dataset & Small      & Medium     & Large        \\ \hline
Original                        & 2050766 $\mu s$ & 4240619 $\mu s$ & 187729378 $\mu s$ \\ \hline
OpenMP                          & 184573 $\mu s$  & 235295 $\mu s$  & 9150758 $\mu s$   \\ \hline
CUDA                            & 124677 $\mu s$  & 155702 $\mu s$  & \textcolor{ugreen}{\textbf{1789467 $\mu s$}}   \\ \hline
Speedup Original / CUDA         & 16.45      & 27.23      & \textcolor{ugreen}{\textbf{104.91}}       \\ \hline
Speedup OpenMP / CUDA           & 1.48       & 1.51       & 5.1          \\ \hline
\end{tabular}
}
\caption{Speedup comparison of the original implementation, the OpenMP implementation and the CUDA implementation}
\label{table:cudatime}
\end{table}

\subsection{Conclusion}
\todo{V}
The task was to parallelize the program, such that it could be executed faster
on either a CPU or a GPU. We have succeeded, and achieved major speedups on 
both the OpenMP implementation and the CUDA implementation. The CUDA implementation
runs over 100 times faster than the original implementation, and it runs 5 times 
faster than the OpenMP implementation, which was already very fast. It would have
been great to try to optimize for the small and the medium datasets, but optimizing
for the large dataset was a priority for us, and it seems it has paid off with a 
very good speedup.

% \subsection{Remaining work}
\todo{
V, small and medium datasets, optimizing for those, constant memory
// Asbjoern: I think this was covered earlier. i vouch for removal of the subsection
// Esben: Yeah, lets just remove it.
}


\newpage
\section{Appendices}
\subsection{Example output}
Here follows example output from running the different implementations.

\subsection{Original implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-small.txt}
\subsection{OpenMP implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-small.txt}
\subsection{CUDA implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-small.txt}

\subsection{Original implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-medium.txt}
\subsection{OpenMP implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-medium.txt}
\subsection{CUDA implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-medium.txt}

\subsection{Original implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-large.txt}
\subsection{OpenMP implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-large.txt}
\subsection{CUDA implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-large.txt}

\end{document}
