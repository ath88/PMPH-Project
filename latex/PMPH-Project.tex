\documentclass[11pt]{article}

%---- defitions ----
\def\Title{Programming Massively Parallel Hardware\\
\vspace{1.5cm}
\textbf{Group Project}}
\def\Author{Esben Skaarup, Asbj\o rn Thegler \& \'{A}sbj\o rn Vider\o \ J\o kladal}

%---- packages ----
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{courier}
% \usepackage{listings}
\usepackage[pdftex,colorlinks=true]{hyperref}
\usepackage{graphics}
\usepackage{todonotes}

\usepackage{color}
\definecolor{ublue}{rgb}{0,0,0.5}
\definecolor{ugreen}{rgb}{0,0.5,0}
\definecolor{ured}{rgb}{0.5,0,0}
\definecolor{ugrey}{rgb}{0.5,0.5,0.5}

\usepackage{listings}
\lstset{
	language=C,			% choose the language of the code
	% numbers=left,				% where to put the line-numbers
	numberstyle=\tiny,			% line-numbers font size
	stepnumber=1,				% the step between two line-numbers
%	numbersep=10pt,				% how far the line-numbers are from the code
	basicstyle= \ttfamily \footnotesize, %\small %\scriptsize, % the size of the fonts for the code
%	breaklines=true,
	tabsize=4,
	identifierstyle=\color{black},
	keywordstyle=\color{ublue}\bf,
	stringstyle=\color{ured},
	commentstyle=\color{ugreen},
	showstringspaces=false,		% underline spaces within strings
	xleftmargin=.1\textwidth, % left margin
	xrightmargin=.1\textwidth, % right margin
%	inputencoding=utf8x,
%	extendedchars= true
%	frame=single,
}

\begin{document}
\title{\Title}
\author{\Author}
\date{\today}
\maketitle

\section{Introduction}
\todo{S V T}
In this assignment, we have parallelized an implementation of volatility calibration using the Crank-Nicolson finite difference method.

We have chosen to first reason about the original sequential implementation. 
Next, we reason about and explain how we created our OpenMP 
implementation. Finally, we reason about our CUDA implementation, and compare
it to the original implementation and the OpenMP implementation.

\section{Sequential Implementation}
\todo{S V T}
First we will profile the original implementation, and showcase our findings.
This will help us determine what we definitely should parallelize, and what is
less relevant.

\subsection{Timing}
\todo{S V T}
We have made approximate timing of specific parts of the original 
implementation, to get an overview 
of where we can achieve relevant speedups according to Amdahl's Law. The results
can be seen in \autoref{table:origtime}. In the table, the indentations 
in the first column indicate what parts are inside other parts. For example,
the running time of tridag\_0 is part of the running time of rollback\_2, which
in turn is part of rollback, and so forth. The time of the outer loop is the 
total running time of the entire calculation. 

\autoref{table:origtime} is based on a single execution of the implementation, 
but we have visually verified that it is a average example. 
The table clearly shows what parts of the implementation we should parallelize.
We see that, combined, the update\_Params function and the rollback function take approximately $56.83\%+42.83\%=99.66\%$ of the execution time.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
Name \textbackslash\ Dataset & Small      & Small \% & Medium     & Medium \% & Large        & Large \% \\ \hline
outer                        & 2050766 us & 100.00\% & 4240619 us & 100.00\%  & 187729378 us & 100.00\% \\ \hline
\ \ init                     & 171 us     & 0.00\%   & 238 us     & 0.00\%    & 2068 us      & 0.00\%   \\ \hline
\ \ setPayoff                & 174 us     & 0.00\%   & 363 us     & 0.00\%    & 10651 us     & 0.01\%   \\ \hline
\ \ updateParams             & 1199969 us & 58.15\%  & 2492123 us & 58.77\%   & 106691216 us & 56.83\%  \\ \hline
\ \ rollback                 & 839027 us  & 40.91\%  & 1729438 us & 40.78\%   & 80412624 us  & 42.83\%  \\ \hline
\ \ \ \ rollback\_0          & 117198 us  & 5.71\%   & 246938 us  & 5.82\%    & 11824914 us  & 6.30\%   \\ \hline
\ \ \ \ rollback\_1          & 111773 us  & 5.45\%   & 236186 us  & 5.56\%    & 12284147 us  & 6.54\%   \\ \hline
\ \ \ \ rollback\_2          & 286646 us  & 13.98\%  & 585957 us  & 13.82\%   & 25915774 us  & 13.80\%  \\ \hline
\ \ \ \ \ \ tridag\_0        & 203364 us  & 9.92\%   & 421277 us  & 9.93\%    & 18500994 us  & 9.86\%   \\ \hline
\ \ \ \ rollback\_3          & 304214 us  & 14.83\%  & 627087 us  & 14.79\%   & 28554403 us  & 15.21\%  \\ \hline
\ \ \ \ \ \ tridag\_1        & 200502 us  & 9.78\%   & 416905 us  & 9.83\%    & 18612169 us  & 9.91\%   \\ \hline
\end{tabular}
}
\caption{Approximate timings of the original implementation}
\label{table:origtime}
\end{table}

A bit of terminology: We refer to the following loop
\begin{lstlisting}[language=C]
  for(unsigned i = 0; i < outer; ++ i) { ... }
\end{lstlisting}
as just the \emph{outer}-loop. This loop
\begin{lstlisting}[language=C]
  for(int i = numT-2;i>=0;--i) { ... }
\end{lstlisting}
we call the \emph{timeline}-loop, where we have defined
\begin{lstlisting}[language=C]
  numT = globs.myTimeline.size().
\end{lstlisting}
And finally, these two loops
\begin{lstlisting}[language=C]
  for(i=0;i<numX;i++) { ... }
  for(j=0;j<numY;j++) { ... }
\end{lstlisting}
are called the \emph{x}-loop and the \emph{y}-loop, respectively.

Basically all of the program runs inside the outer-loop.
In this loop, first there is some initialization, and then we enter the timeline-loop in which the update\_Params function and the rollback function are called.
Here, the update\_Params function consists of a nested for-loop (an y-loop inside an x-loop), and the rollback function runs several combinations of the x-loop and the y-loop, some of which call the tridag function.

So our efforts have been put into finding out which of these loops are (or can be made) parallel, and, in the case of the CUDA implementation, distributing and interchanging the parallel loops in order to create perfect loop-nests, which then correspond to kernels.

// TODO explain more ??

\subsection{Validation}
\todo{S V T}
The original implementation validates against all 3 datasets. This is expected,
since we did not change the original implementation. It is assumed that the 
original implementation is therefore correct.


\section{OpenMP Implementation}
\todo{S V T}
In this section we will first reason about how we transformed the sequential
implementation to run on multiple cores. Then we will reason about the validity 
of our implementation, and finally we will compare it to the original 
implementation.

\subsection{Privatization}
\todo{S V T}
The outer-loop in the original implementation uses the same C 
struct for each
iteration of the loop. This, in turn, makes the loop inherently non-parallel, since,
if executed in parallel, all iterations would be writing to the same memory location. 
We can, however, give each iteration its own copy of the
struct, so that the iterations do not read and write to the same memory location. This is 
known as privatization, and allows us to parallelize the entire loop
with an OpenMP pragma directive. 

While privatization does work in this case, it is not immediately clear why it does. It is
important to note that it would not have worked if there had been a dependency between the iterations,
e.g., an accumulator that was used by the next iteration. The only way to ensure
that this is not the case is to look through the code and take note of which 
variables are read, and verify that they are all previously written in the 
same iteration, such that nothing is carried from each loop iteration.

When telling OpenMP how to parallelize, we decided to go with static scheduling.
This means that all iterations are distributed before executing the loop. This gives
less overhead, compared to dynamic scheduling, which adjusts to the workload at runtime. We do
this because we know that each iteration of the outer loop yields the same
amount of work, such that no iteration takes much longer time than the others.
Had they been of different size, then static scheduling would lead to load 
imbalance, and incur extra overhead due to significant idling on some cores.


\subsection{Validation}
\todo{S V T}
The implementation validates against all 3 datasets. This shows that our
implementation is not catastrophically wrong, but does not \emph{prove} that it is
correct. However, we make the same assumption about correctness as with the 
original implementation: The fact that it validates is sufficient evidence of correctness.

We have briefly looked at how the algorithm works and what it does. We
realize that if we have a wrong implementation, then the results should differ quite much from the correct results, provided we have a relatively high amount of timesteps. We created our own 'Tiny' dataset and ran it on the original implementation to produce ``correct'' output data for it. It turned out that some implementations validated on this dataset, even though they actually failed on the other datasets.

Another curiosity is that, if we skip writing to the result array, then there
is a good chance that we end up allocating the exact same block of memory as the 
previous execution, and that the results are already there. This happened to us
at least once, and it can be fixed by 0-initializing the result array prior to
the calculation.


\subsection{Speedup}
\todo{S V T}
While this implementation is parallel across multiple processors, measuring the
time taking of specific parts of the implementation is harder. We can, however, 
compare with the total running time, and the results can be seen in 
\autoref{table:omptime}.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|}
\hline
Speedup \textbackslash\ Dataset & Small      & Medium     & Large        \\ \hline
Original                        & 2050766 us & 4240619 us & 187729378 us \\ \hline
OpenMP                          & 184573 us  & 235295 us  & 9150758 us   \\ \hline
Speedup                         & 11.11      & 18.02      & 20.52        \\ \hline
\end{tabular}
}
\caption{Speedup comparison of the original implementation and the OpenMP implementation}
\label{table:omptime}
\end{table}

\subsection{Conclusion}
\todo{S V T}
When using OpenMP, we utilize the machine's 16 cores with hyperthreading. 
This will give us a maximum realistic speedup of factor 32. When running the small 
dataset, the OpenMP pragma we used will only create 16 processes on 16 cores
corresponding to the number of iterations on the outer loop. This means that 
we will only get a max speedup of factor 16 on the small dataset. 

When using 16 cores, we expect a speedup of slightly less than 16. Then we 
gain a little extra from hyperthreading, if we have more than 16 processes. 
Finally we loose a little as overhead from creating and sharing result memory.
This makes the results very much in line with our expectations.


\section{CUDA Implementation}
\todo{S V T}
In this section we will first reason about how we transformed the sequential
implementation to run on the GPU. Then we will reason about the validity of 
our implementation, and finally we will compare it to the original 
implementation and the OpenMP implementation.

\subsection{Angle of Approach}
\todo{S V T, we need to explain why we only parallelize across O and Y}
// Prøv og se om \autoref{sec:approach} besvarer dette.

\subsection{Transformations}
\todo{S V T}
We did various transpormations on the original code. Here we explain what we 
did in approximately chronological order.

\subsubsection{Array Expansion}
\todo{S V T}
The outer loop is not parallel as it is. Previously we remedied this by privatization,
but this approach will not work here, since we aim to use the GPU where
it is preferable to do all memory allocation before launching the CUDA-kernels.
In addition, we need to keep variables available across multiple kernels. Therefore we
expand most of the arrays in the struct with O copies, where O corresponds to
the number of iterations in the outer loop. The arrays we have expanded are:
myResult, myVarX, myVarY, u, v, a, b, c, y and yy. Those we chosen because 
they are all being written to inside the loop. The remaining arrays are only 
read from, and will thus not result in a conflict. 

After the array expansion,
it was necessary to ensure that each iteration used its own part of the newly
expanded array. This is done by indexing into the expanded arrays, based on the
iteration counter, for each iteration. When the indexing into the expanded
array is correct, then we have successfully made the outer-loop parallel.

\subsubsection{Loop Distribution}
\todo{S V T}
When we have established that a loop is parallel, we can make use of the theorem which says that a parallel loop can be distributed across its statements.
So if we have a parallel loop containing statements A, B and C, we can transform this into three separate loops ---
one that loops over A, a second one that loops over B, and a third one that loops over C.
The only condition is that we must remember to array-expand all private variables that span two or more of the newly created loops (otherwise, each iteration of one loop would just overwrite the variable such that only the value from the last thread to finish would be available in the next loop, which is obviously not what we want).

At this point, we have a parallel outer-loop that has some initialization statements (initGrid and initOperator), and then a sequential loop (the timeline-loop).
So we distribute the outer-loop, giving us one outer-loop containing the initialization statements, and another outer-loop containing just the timeline-loop.
After distributing, we can continue the transformations on both new copies of the outer-loop.

% how we can run all iterations of parts of a parallel loop at the same time

\subsubsection{Loop Interchange}
\todo{S V T}
At this point in our sequence of transformations, in addition to an initialization loop, the program consists of the parallel outer-loop with the sequential timeline-loop inside of it.
Now we can make use of the fact that a parallel loop can be interchanged inwards.
So we do a loop interchange, such that the sequential timeline-loop now becomes the outermost loop.
Now we have the parallel outer-loop on the inside of the sequential loop (note that we still call it the ``outer''-loop), and then we can continue applying loop distribution and loop interchange until our program has the desired structure.
As previously mentioned, we want to end up with perfect loop-nests that correspond to CUDA-kernels.

\subsubsection{Approach to Parallelization}
\label{sec:approach}
Actually, the goal isn't necessarily to end up with loop-nests \emph{per se}. So to put it more precise, we want parallel loops with enough iterations (i.e., threads) to fully utilize the GPU.
So single loops are also perfectly fine, as long as their sizes are large enough.
In our datasets, however, no single dimension is large enough to fully occupy the GPU, so we do need to create loop-nests in order to obtain sufficiently large loops.

Currently, we have only optimized our program towards the large dataset.
After performing the transformations mentioned above, in much of the code we end up with triple-nested loops: An outer-loop, an x-loop and a y-loop. Since each of these loops is parallel, such nests might be interchanged in any way we want. For the large dataset, it is sufficient to parallelize along two of the three dimensions. Two loops are large enough to fully utilize the GPU, and further parallelization would not increase the amount of work done in parallel (/* Måske nogle beregninger her? Og ville det forværre performance (mere overhead) at parallelisere mere? */). This is something we have only reasoned about with pen and paper calculations --- we didn't do any experiment parallelizing the third dimension to see what the actual impact was.

As a consequence, we have only made two-dimensional kernels at this point. We have chosen to parallelize along the outer and y dimensions where possible. Some places in the code, the transformations only gave doubly-nested loops, and if those consisted of an outer-loop and an x-loop, then naturally we parallelize along the outer and x dimensions instead.

// Esben, kan du uddybe f.eks. hvor meget der skal til for at fylde GPU'en?

%This allows us to move the outer loop 
%inwards, such that the parallel loops are in close proximity to each other. 
%Later, this will allow us to easily create kernels, since this activity is 
%trivial when the loops are in close proximity.

\subsubsection{Coalesced Data Access}
After having parallelized the execution, the data accesses were a major bottleneck because the they were not coalesced.
The problems were mostly $x$ by $y$ matrices that were traversed in the opposite order on the GPU because we have parallelized the $y$ loop.
When traversing a matrix the access pattern should look like this (where \texttt{i} traverses the $x$-axis in the loop and \texttt{j} traverses the $y$-axis in parallel):
\begin{lstlisting}[language=C]
for(int i = 0; i < numX; i++) {
	data[i*numY + j];
}
\end{lstlisting}

Since most of the matrices are always traversed in the same direction, we could simply swap the indexing at every access to the matrix, resulting in major speedups without adding any code.

Two matrices, namely \texttt{myResult} and \texttt{y} were, however, traversed in different directions at different places in the loop.
For these, we inserted kernels to transpose them between kernel calls, so that each kernel can traverse the data in an optimal manner and still get the desired results.
We used the tiled transpose kernel from assignment 3 for this (modified to take the outer loop into account), and the time spent transposing is much less than the time saved using coalesced access.
Only on the small data set is the time spent transposing a dominating contribution to the total time.

\todo{S V T}

\subsubsection{Nudging around}
\todo{S V T}

\paragraph{Duplicate Data Access\\}
At several places in the code, some data was accessed several times close to each other, but not close enough to be optimized by the compiler.
We obtained significant speedups by caching this data in a local variable.

Similarly, there were places where data close to each other was read throughout a kernel.

\paragraph{Shared Memory\\}
\paragraph{Block Dimensions\\}
\paragraph{Shared memory\\}
\paragraph{Constant Memory Indexing\\}
\paragraph{Array Indexing\\}
indexing into arrays, dont do it like Cosmin wants us to.

\subsection{Validation}
\todo{S V T}
This implementation validates against all 3 datasets. This shows that our
implementation is not catastrophically wrong, but does not prove that it is
correct. However, we make the same assumption about correctness as with the 
original implementation and the OpenMP implementation: The fact that it 
validates is sufficient evidence of correctness.

\subsection{Speedup}
\todo{S V T}


\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|}
\hline
Speedup \textbackslash\ Dataset & Small      & Medium     & Large        \\ \hline
Original                        & 2050766 us & 4240619 us & 187729378 us \\ \hline
OpenMP                          & 184573 us  & 235295 us  & 9150758 us   \\ \hline
CUDA                            & 124677 us  & 155702 us  & 1789467 us   \\ \hline
Speedup Original / CUDA         & 16.45      & 27.23      & \textcolor{ugreen}{\textbf{104.91}}       \\ \hline
Speedup OpenMP / CUDA           & 1.48       & 1.51       & 5.1          \\ \hline
\end{tabular}
}
\caption{Speedup comparison of the original implementation, the OpenMP implementation and the CUDA implementation}
\label{table:cudatime}
\end{table}

\subsection{Conclusion}
\todo{S V T}

\subsection{Remaining work}
\todo{S V T, small and medium datasets, optimizing for those, constant memory}


\newpage
\section{Appendices}
\subsection{Example output}
Here follows example output from running the different implementations.

\subsection{Original implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-small.txt}
\subsection{OpenMP implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-small.txt}
\subsection{CUDA implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-small.txt}

\subsection{Original implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-medium.txt}
\subsection{OpenMP implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-medium.txt}
\subsection{CUDA implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-medium.txt}

\subsection{Original implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-large.txt}
\subsection{OpenMP implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-large.txt}
\subsection{CUDA implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-large.txt}

\end{document}
