\documentclass[11pt]{article}

%---- defitions ----
\def\Title{Programming Massively Parallel Hardware\\
\vspace{1.5cm}
\textbf{Group Project}}
\def\Author{Esben Skaarup, Asbj\o rn Thegler \& \'{A}sbj\o rn Vider\o \ J\o kladal}

%---- packages ----
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{courier}
\usepackage{listings}
\usepackage[pdftex,colorlinks=true]{hyperref}
\usepackage{graphics}

\begin{document}
\title{\Title}
\author{\Author}
\date{\today}
\maketitle

We have chosen to first reason about the original sequential implementation. 
Next, we reason about and explain how we solved the OpenMP 
implementation. Finally, we reason about our CUDA implementation, and measure
it against the original implementation and the OpenMP implementation.

\section{Sequential Implementation}
\subsection{Timing}
We have made approximate timing of specific parts of the original 
implementation, to get an overview 
of where we can achieve relevant speedups according to Amdahl's Law. The results
can be seen in \autoref{table:origtime}. In the table, there are indentations 
in the first column indicating what parts are inside other parts. For example,
the running time of tridag\_0 is part of the running time of rollback\_2, which
in turn is part of rollback, and so forth. The time of the outer loop is the 
total running time of the entire calculation. 

\autoref{table:origtime} is based on a single execution of the implementation, 
but we have visually verified that it is a statistically average example. 
The table clearly shows what parts of the implementation we should parallelize.
We see that, combined, the update\_Params function and the rollback function take approximately $56.83\%+42.83\%=99.66\%$ of the execution time.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
Name \textbackslash\ Dataset & Small      & Small \% & Medium     & Medium \% & Large        & Large \% \\ \hline
outer                        & 2050766 us & 100.00\% & 4240619 us & 100.00\%  & 187729378 us & 100.00\% \\ \hline
\ \ init                     & 171 us     & 0.00\%   & 238 us     & 0.00\%    & 2068 us      & 0.00\%   \\ \hline
\ \ setPayoff                & 174 us     & 0.00\%   & 363 us     & 0.00\%    & 10651 us     & 0.01\%   \\ \hline
\ \ updateParams             & 1199969 us & 58.15\%  & 2492123 us & 58.77\%   & 106691216 us & 56.83\%  \\ \hline
\ \ rollback                 & 839027 us  & 40.91\%  & 1729438 us & 40.78\%   & 80412624 us  & 42.83\%  \\ \hline
\ \ \ \ rollback\_0          & 117198 us  & 5.71\%   & 246938 us  & 5.82\%    & 11824914 us  & 6.30\%   \\ \hline
\ \ \ \ rollback\_1          & 111773 us  & 5.45\%   & 236186 us  & 5.56\%    & 12284147 us  & 6.54\%   \\ \hline
\ \ \ \ rollback\_2          & 286646 us  & 13.98\%  & 585957 us  & 13.82\%   & 25915774 us  & 13.80\%  \\ \hline
\ \ \ \ \ \ tridag\_0        & 203364 us  & 9.92\%   & 421277 us  & 9.93\%    & 18500994 us  & 9.86\%   \\ \hline
\ \ \ \ rollback\_3          & 304214 us  & 14.83\%  & 627087 us  & 14.79\%   & 28554403 us  & 15.21\%  \\ \hline
\ \ \ \ \ \ tridag\_1        & 200502 us  & 9.78\%   & 416905 us  & 9.83\%    & 18612169 us  & 9.91\%   \\ \hline
\end{tabular}
}
\caption{Approximate timings of the original implementation}
\label{table:origtime}
\end{table}

A bit of terminology: We refer to the following loop
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,breaklines=true]
  for(unsigned i = 0; i < outer; ++ i) { ... }
\end{lstlisting}
as just the \emph{outer}-loop. This loop
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,breaklines=true]
  for(int i = numT-2;i>=0;--i) { ... }
\end{lstlisting}
we call the \emph{timeline}-loop, where we have defined
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,breaklines=true]
  numT = globs.myTimeline.size().
\end{lstlisting}
And finally, these two loops
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,breaklines=true]
  for(i=0;i<numX;i++) { ... }
  for(j=0;j<numY;j++) { ... }
\end{lstlisting}
are called the \emph{x}-loop and the \emph{y}-loop, respectively.

Basically all of the program runs inside the outer-loop.
In this loop, first there is some initialization, and then we enter the timeline-loop in which the update\_Params function and the rollback function are called.
Here, the update\_Params function consists of a nested for-loop (an y-loop inside an x-loop), and the rollback function runs several combinations of the x-loop and the y-loop, some of which call the tridag function.

So to our efforts have been put into finding out which of these loops are (or can be made) parallel, and in the case of the CUDA implementation we have distributed and interchanged the loops in order to create perfect loop-nests, which then correspond to kernels.

TODO explain more

\subsection{Validation}
The original implementation validates against all 3 datasets. This is expected,
since we did not change the original implementation. It is assumed that the 
original implementation is therefore correct.


\section{OpenMP Implementation}
\subsection{Privatization}
The outer loop in the original implementation uses the same allocation of a C 
struct for each
iteration of the loop. This, in turn, makes it inherently not parallel, since,
if executed in parallel, all iterations would be writing to the same memory. 
We can, however, privatize the struct, such that all iterations allocate their
own struct, and therefore does not read and write to the same memory. This is 
a method known as privatization, and allows us to parallelize the entire loop
with an OpenMP pragma directive. 

While privatization does work in this case, this is not immediately clear. It is
important to note that this would not have worked, if there had been some kind
of accumulator in the struct, used by the next iteration. The only way to ensure
that this is not the case, is to look through the code and take note of which 
variables are read, and realize that they are all previously written in the 
same iteration, such that nothing is carried from each loop iteration.

TODO static scheduling


\subsection{Validation}
The implementation validates against all 3 datasets. This shows that our
implementation is not catastrophically wrong, but does not prove that it is
correct. We can make the same assumption about correctness as with the 
original implementation.

We have shortly been looking into how the algorithm works, and what it does. We
realize that if we have a wrong implementation, and a relatively high amount of 
timesteps, then the results will be very off from the correct results. We have,
however, created our own 'Tiny' dataset, which turned out to validate on some
implementations, but fail on others.

\subsection{Speedup}
While this implementation is parallel across multiple processors, measuring the
time taking of specific parts of the implementation is harder. We can, however, 
compare with the total running time, and the results can be seen in 
\autoref{table:omptime}.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|}
\hline
Speedup \textbackslash\ Dataset & Small      & Medium     & Large        \\ \hline
Original                        & 2050766 us & 4240619 us & 187729378 us \\ \hline
OpenMP                          & 184573 us  & 235295 us  & 9150758 us   \\ \hline
Speedup                         & 11.11      & 18.02      & 20.52        \\ \hline
\end{tabular}
}
\caption{Approximate timings of the original implementation}
\label{table:omptime}
\end{table}

\subsection{Conclusion}
When using OpenMP, we utilize the machines 16 cores with dual hyperthreading. 
This will give us a theoretical max speedup of factor 32. When running the small 
dataset, the OpenMP pragma we used will only create 16 processes on 16 cores
corresponding to the number of iterations on the outer loop. This means that 
we will only get a theoretical max speedup of factor 16, on the small dataset. 

While using 16 cores, we expect a speedup of slight amount short of 16. Then we 
gain a little extra from hyperthreading, if we have more than 16 processes. 
Finally we loose a little from overhead from creating and sharing result memory.



\section{CUDA Implementation}

\subsection{Transformations}
TODO indexing into arrays, dont do it like Cosmin wants us to. Esben?
\subsubsection{Loop Distribution}
\subsubsection{Coalesced Data Access}
\subsubsection{Loop Interchange}
\subsubsection{Array Expansion}


\subsection{Validation}
\subsection{Speedup}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|}
\hline
Speedup \textbackslash\ Dataset & Small      & Medium     & Large        \\ \hline

Original                        & 2050766 us & 4240619 us & 187729378 us \\ \hline
OpenMP                          & 184573 us  & 235295 us  & 9150758 us   \\ \hline
CUDA                            &  us  &  us  &  us   \\ \hline
Speedup                         & 11.11      & 18.02      & 20.52        \\ \hline
\end{tabular}
}
\caption{Approximate timings of the original implementation}
\label{table:omptime}
\end{table}
\subsection{Conclusion}

\newpage
\section{Appendices}
\subsection{Example output}
Here follows example output from running the different implementations.

\subsection{Original implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-small.txt}
\subsection{OpenMP implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-small.txt}
\subsection{Original implementation, medium dataset}

\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-medium.txt}
\subsection{OpenMP implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-medium.txt}
\subsection{Original implementation, large dataset}

\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-large.txt}
\subsection{OpenMP implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-large.txt}

\end{document}
