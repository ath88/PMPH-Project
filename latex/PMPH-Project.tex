\documentclass[11pt]{article}

%---- defitions ----
\def\Title{Programming Massively Parallel Hardware\\
\vspace{1.5cm}
\textbf{Group Project}}
\def\Author{Esben Skaarup, Asbj\o rn Thegler \& \'{A}sbj\o rn Vider\o \ J\o kladal}

%---- packages ----
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{courier}
% \usepackage{listings}
\usepackage[pdftex,colorlinks=true]{hyperref}
\usepackage{graphics}
\usepackage{todonotes}

\usepackage{color}
\definecolor{ublue}{rgb}{0,0,0.5}
\definecolor{ugreen}{rgb}{0,0.5,0}
\definecolor{ured}{rgb}{0.5,0,0}
\definecolor{ugrey}{rgb}{0.5,0.5,0.5}

\usepackage{listings}
\lstset{
	language=C,			% choose the language of the code
	% numbers=left,				% where to put the line-numbers
	numberstyle=\tiny,			% line-numbers font size
	stepnumber=1,				% the step between two line-numbers
%	numbersep=10pt,				% how far the line-numbers are from the code
	basicstyle= \ttfamily \footnotesize, %\small %\scriptsize, % the size of the fonts for the code
%	breaklines=true,
	tabsize=4,
	identifierstyle=\color{black},
	keywordstyle=\color{ublue}\bf,
	stringstyle=\color{ured},
	commentstyle=\color{ugreen},
	showstringspaces=false,		% underline spaces within strings
	xleftmargin=.1\textwidth, % left margin
	xrightmargin=.1\textwidth, % right margin
%	inputencoding=utf8x,
%	extendedchars= true
%	frame=single,
}

\begin{document}
\title{\Title}
\author{\Author}
\date{\today}
\maketitle

\todo{S V}
We have chosen to first reason about the original sequential implementation. 
Next, we reason about and explain how we created our OpenMP 
implementation. Finally, we reason about our CUDA implementation, and compare
it to the original implementation and the OpenMP implementation.

\section{Sequential Implementation}
\todo{S V T}
First we will profile the original implementation, and showcase our findings.
This will help us determine what we definitely should parallelize, and what is
less relevant.

\subsection{Timing}
\todo{S V T}
We have made approximate timing of specific parts of the original 
implementation, to get an overview 
of where we can achieve relevant speedups according to Amdahl's Law. The results
can be seen in \autoref{table:origtime}. In the table, the indentations 
in the first column indicate what parts are inside other parts. For example,
the running time of tridag\_0 is part of the running time of rollback\_2, which
in turn is part of rollback, and so forth. The time of the outer loop is the 
total running time of the entire calculation. 

\autoref{table:origtime} is based on a single execution of the implementation, 
but we have visually verified that it is a average example. 
The table clearly shows what parts of the implementation we should parallelize.
We see that, combined, the update\_Params function and the rollback function take approximately $56.83\%+42.83\%=99.66\%$ of the execution time.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
Name \textbackslash\ Dataset & Small      & Small \% & Medium     & Medium \% & Large        & Large \% \\ \hline
outer                        & 2050766 us & 100.00\% & 4240619 us & 100.00\%  & 187729378 us & 100.00\% \\ \hline
\ \ init                     & 171 us     & 0.00\%   & 238 us     & 0.00\%    & 2068 us      & 0.00\%   \\ \hline
\ \ setPayoff                & 174 us     & 0.00\%   & 363 us     & 0.00\%    & 10651 us     & 0.01\%   \\ \hline
\ \ updateParams             & 1199969 us & 58.15\%  & 2492123 us & 58.77\%   & 106691216 us & 56.83\%  \\ \hline
\ \ rollback                 & 839027 us  & 40.91\%  & 1729438 us & 40.78\%   & 80412624 us  & 42.83\%  \\ \hline
\ \ \ \ rollback\_0          & 117198 us  & 5.71\%   & 246938 us  & 5.82\%    & 11824914 us  & 6.30\%   \\ \hline
\ \ \ \ rollback\_1          & 111773 us  & 5.45\%   & 236186 us  & 5.56\%    & 12284147 us  & 6.54\%   \\ \hline
\ \ \ \ rollback\_2          & 286646 us  & 13.98\%  & 585957 us  & 13.82\%   & 25915774 us  & 13.80\%  \\ \hline
\ \ \ \ \ \ tridag\_0        & 203364 us  & 9.92\%   & 421277 us  & 9.93\%    & 18500994 us  & 9.86\%   \\ \hline
\ \ \ \ rollback\_3          & 304214 us  & 14.83\%  & 627087 us  & 14.79\%   & 28554403 us  & 15.21\%  \\ \hline
\ \ \ \ \ \ tridag\_1        & 200502 us  & 9.78\%   & 416905 us  & 9.83\%    & 18612169 us  & 9.91\%   \\ \hline
\end{tabular}
}
\caption{Approximate timings of the original implementation}
\label{table:origtime}
\end{table}

A bit of terminology: We refer to the following loop
\begin{lstlisting}[language=C]
  for(unsigned i = 0; i < outer; ++ i) { ... }
\end{lstlisting}
as just the \emph{outer}-loop. This loop
\begin{lstlisting}[language=C]
  for(int i = numT-2;i>=0;--i) { ... }
\end{lstlisting}
we call the \emph{timeline}-loop, where we have defined
\begin{lstlisting}[language=C]
  numT = globs.myTimeline.size().
\end{lstlisting}
And finally, these two loops
\begin{lstlisting}[language=C]
  for(i=0;i<numX;i++) { ... }
  for(j=0;j<numY;j++) { ... }
\end{lstlisting}
are called the \emph{x}-loop and the \emph{y}-loop, respectively.

Basically all of the program runs inside the outer-loop.
In this loop, first there is some initialization, and then we enter the timeline-loop in which the update\_Params function and the rollback function are called.
Here, the update\_Params function consists of a nested for-loop (an y-loop inside an x-loop), and the rollback function runs several combinations of the x-loop and the y-loop, some of which call the tridag function.

So our efforts have been put into finding out which of these loops are (or can be made) parallel, and, in the case of the CUDA implementation, distributing and interchanging the loops in order to create perfect loop-nests, which then correspond to kernels.

// TODO explain more ??

\subsection{Validation}
\todo{S V T}
The original implementation validates against all 3 datasets. This is expected,
since we did not change the original implementation. It is assumed that the 
original implementation is therefore correct.


\section{OpenMP Implementation}
\todo{S V T}
In this section we will first reason about how we transformed the sequential
implementation to run on multiple cores. Then we will reason about the validity 
of our implementation, and finally we will compare it to the original 
implementation.

\subsection{Privatization}
\todo{S V T}
The outer-loop in the original implementation uses the same C 
struct for each
iteration of the loop. This, in turn, makes the loop inherently non-parallel, since,
if executed in parallel, all iterations would be writing to the same memory location. 
We can, however, give each iteration its own copy of the
struct, so that the iterations do not read and write to the same memory location. This is 
known as privatization, and allows us to parallelize the entire loop
with an OpenMP pragma directive. 

While privatization does work in this case, it is not immediately clear why it does. It is
important to note that it would not have worked if there had been a dependency between the iterations,
e.g., an accumulator that was used by the next iteration. The only way to ensure
that this is not the case is to look through the code and take note of which 
variables are read, and verify that they are all previously written in the 
same iteration, such that nothing is carried from each loop iteration.

When telling OpenMP how to parallelize, we decided to go with static scheduling.
This means that all iterations are distributed before executing the loop. This gives
less overhead, compared to dynamic scheduling, which adjusts to the workload at runtime. We do
this because we know that each iteration of the outer loop yields the same
amount of work, such that no iteration takes much longer time than the others.
Had they been of different size, then static scheduling would lead to load 
imbalance, and incur extra overhead due to significant idling on some cores.


\subsection{Validation}
\todo{S V T}
The implementation validates against all 3 datasets. This shows that our
implementation is not catastrophically wrong, but does not prove that it is
correct. However, we make the same assumption about correctness as with the 
original implementation: The fact that it validates is sufficient evidence of correctness.

We have briefly looked at how the algorithm works and what it does. We
realize that if we have a wrong implementation, then the results should differ quite much from the correct results, provided we have a relatively high amount of timesteps. We created our own 'Tiny' dataset and ran it on the original implementation to produce ``correct'' output data for it. It turned out that some implementations validated on this dataset, even though they actually failed on the other datasets.

Another curiosity is that, if we skip writing to the result array, then there
is a good chance that we allocated the exact same block of memory as the 
previous execution, and that the results are already there. This happened to us
at least once, and it can be fixed by 0-initializing the result array, prior to
the calculation.


\subsection{Speedup}
\todo{S V T}
While this implementation is parallel across multiple processors, measuring the
time taking of specific parts of the implementation is harder. We can, however, 
compare with the total running time, and the results can be seen in 
\autoref{table:omptime}.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|}
\hline
Speedup \textbackslash\ Dataset & Small      & Medium     & Large        \\ \hline
Original                        & 2050766 us & 4240619 us & 187729378 us \\ \hline
OpenMP                          & 184573 us  & 235295 us  & 9150758 us   \\ \hline
Speedup                         & 11.11      & 18.02      & 20.52        \\ \hline
\end{tabular}
}
\caption{Speedup comparison of the original implementation and the OpenMP implementation}
\label{table:omptime}
\end{table}

\subsection{Conclusion}
\todo{S V T}
When using OpenMP, we utilize the machines 16 cores with hyperthreading. 
This will give us a maximum realistic speedup of factor 32. When running the small 
dataset, the OpenMP pragma we used will only create 16 processes on 16 cores
corresponding to the number of iterations on the outer loop. This means that 
we will only get a max speedup of factor 16, on the small dataset. 

While using 16 cores, we expect a speedup of slight amount short of 16. Then we 
gain a little extra from hyperthreading, if we have more than 16 processes. 
Finally we loose a little as overhead from creating and sharing result memory.
This makes the results very much in line with our expectations.


\section{CUDA Implementation}
\todo{S V T}
In this section we will first reason about how we transformed the sequential
implementation to run on the GPU. Then we will reason about the validity of 
our implementation, and finally we will compare it to the original 
implementation and the OpenMP implementation.

\subsection{Angle of Approach}
\todo{S V T, we need to explain why we only parallelize across O and Y}

\subsection{Transformations}
\todo{S V T}
We did various transpormations on the original code. Here we explain what we 
did in approximately chronological order.

\subsubsection{Array Expansion}
\todo{S V T}
The outer loop is not parallel. Previously we remedied this by privatization,
but this approach will not work here, since we aim to use the GPU, and will 
need to keep the struct available across multiple CUDA kernels. Therefore we
expand most of the arrays in the struct with O copies, where O corresponds to
the number of iterations in the outer loop. The arrays we have expanded are:
myResult, myVarX, myVarY, u, v, a, b, c, y and yy. Those we chosen because 
they are all being written to inside the loop. The remaining arrays are only 
read from, and will thus not result in a conflict. 

After the array expansion,
it was necessary to ensure that each iteration used its own part of the newly
expanded array. This is done by indexing into the expanded arrays, based on the
iteration counter, for each iteration. When the indexing into the expanded
array is correct, then we have successfully made the outer loop fully parallel.


\subsubsection{Loop Interchange}
\todo{S V T}
At this point we have already achieved a parallel outer loop, and a sequential 
timeline loop. Now we can do loop interchanging, such that the sequential 
timeline loop becomes the outermost loop. This allows us to move the outer loop 
inwards, such that the parallel loops are in close proximity to each other. 
Later, this will allow us to easily create kernels, since this activity is 
trivial when the loops are in close proximity.

\subsubsection{Loop Distribution}
\todo{S V T}
how we can run all iterations of parts of a parallel loop at the same time


\subsubsection{Coalesced Data Access}
After having parallelized the execution, the data accesses were a major bottleneck because the they were not coalesced.
The problems were mostly $x$ by $y$ matrices that were traversed in the opposite order on the GPU because we have parallelized the $y$ loop.
When traversing a matrix the access pattern should look like this (where \texttt{i} traverses the $x$-axis in the loop and \texttt{j} traverses the $y$-axis in parallel):
\begin{lstlisting}[language=C]
for(int i = 0; i < numX; i++) {
	data[i*numY + j];
}
\end{lstlisting}

Since most of the matrices are always traversed in the same direction, we could simply swap the indexing at every access to the matrix, resulting in major speedups without adding any code.

Two matrices, namely \texttt{myResult} and \texttt{y} were, however, traversed in different directions at different places in the loop.
For these, we inserted kernels to transpose them between kernel calls, so that each kernel can traverse the data in an optimal manner and still get the desired results.
We used the tiled transpose kernel from assignment 3 for this (modified to take the outer loop into account), and the time spent transposing is much less than the time saved using coalesced access.
Only on the small data set is the time spent transposing a dominating contribution to the total time.

\todo{S V T}
Transposing, flipping axes, etc

\subsubsection{Nudging around}
\todo{S V T}
\todo{indexing into arrays, dont do it like Cosmin wants us to. Esben?}

\subsection{Validation}
\todo{S V T}
This implementation validates against all 3 datasets. This shows that our
implementation is not catastrophically wrong, but does not prove that it is
correct. However, we make the same assumption about correctness as with the 
original implementation and the OpenMP implementation: The fact that it 
validates is sufficient evidence of correctness.

\subsection{Speedup}
\todo{S V T}


\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|r|r|r|}
\hline
Speedup \textbackslash\ Dataset & Small      & Medium     & Large        \\ \hline
Original                        & 2050766 us & 4240619 us & 187729378 us \\ \hline
OpenMP                          & 184573 us  & 235295 us  & 9150758 us   \\ \hline
CUDA                            & 124677 us  & 155702 us  & 1789467 us   \\ \hline
Speedup Original / CUDA         & 16.45      & 27.23      & \textcolor{ugreen}{\textbf{104.91}}       \\ \hline
Speedup OpenMP / CUDA           & 1.48       & 1.51       & 5.1          \\ \hline
\end{tabular}
}
\caption{Speedup comparison of the original implementation, the OpenMP implementation and the CUDA implementation}
\label{table:cudatime}
\end{table}

\subsection{Conclusion}
\todo{S V T}

\newpage
\section{Appendices}
\subsection{Example output}
Here follows example output from running the different implementations.

\subsection{Original implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-small.txt}
\subsection{OpenMP implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-small.txt}
\subsection{CUDA implementation, small dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-small.txt}

\subsection{Original implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-medium.txt}
\subsection{OpenMP implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-medium.txt}
\subsection{CUDA implementation, medium dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-medium.txt}

\subsection{Original implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/orig-run-large.txt}
\subsection{OpenMP implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/OpenMP-run-large.txt}
\subsection{CUDA implementation, large dataset}
\lstinputlisting[basicstyle=\footnotesize\ttfamily,breaklines=true]{data/CUDA-run-large.txt}

\end{document}
